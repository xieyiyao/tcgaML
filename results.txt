kNN with no features deleted
kNN: p = 1 neighbors = 3
Score: 0.87219343696
kNN: p = 2 neighbors = 3
Score: 0.834196891192
kNN: p = 3 neighbors = 3
Score: 0.701208981002
kNN: p = 1 neighbors = 5
Score: 0.860103626943
kNN: p = 2 neighbors = 5
Score: 0.834196891192
kNN: p = 3 neighbors = 5
Score: 0.715025906736

kNN with low variance features deleted
kNN: p = 1 neighbors = 3
Score: 0.87219343696
kNN: p = 2 neighbors = 3
Score: 0.834196891192
kNN: p = 3 neighbors = 3
Score: 0.701208981002
kNN: p = 1 neighbors = 5
Score: 0.860103626943
kNN: p = 2 neighbors = 5
Score: 0.834196891192
kNN: p = 3 neighbors = 5
Score: 0.715025906736

logistic (low variance features deleted highest so far): 0.972366


<bound method LogisticRegression.get_params of LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='multinomial',
          n_jobs=1, penalty='l1', random_state=None, solver='saga',
          tol=0.0001, verbose=0, warm_start=False)>

Accuracy score for test is: 0.963731
Strength is: 1.00,10,100
F-1 score(micro) for test is: 0.9637305699
F-1 score(macro) for test is: 0.9575504159


//sag and saga give same results
<bound method LogisticRegression.get_params of LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='multinomial',
          n_jobs=1, penalty='l2', random_state=None, solver='saga',
          tol=0.0001, verbose=0, warm_start=False)>

Accuracy score for test is: 0.963731
Strength is: 0.01,0.10, 1, 10.00
F-1 score(micro) for test is: 0.9637305699
F-1 score(macro) for test is: 0.9575504159


<bound method LogisticRegression.get_params of LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='saga', tol=0.0001,
          verbose=0, warm_start=False)>
Accuracy score for test is: 0.970639
Strength is: 1.00
F-1 score(micro) for test is: 0.9706390328
F-1 score(macro) for test is: 0.9647441259

<bound method LogisticRegression.get_params of LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='saga', tol=0.0001,
          verbose=0, warm_start=False)>
Accuracy score for test is: 0.967185
Strength is: 1.00
F-1 score(micro) for test is: 0.9671848014
F-1 score(macro) for test is: 0.9558943399

<bound method LogisticRegression.get_params of LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,
          verbose=0, warm_start=False)>
Accuracy score for test is: 0.965458
Strength is: 1.00
F-1 score(micro) for test is: 0.9654576857
F-1 score(macro) for test is: 0.9583198237

<bound method LogisticRegression.get_params of LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='multinomial',
          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',
          tol=0.0001, verbose=0, warm_start=False)>
Accuracy score for test is: 0.956822
Strength is: 1.00
F-1 score(micro) for test is: 0.9568221071
F-1 score(macro) for test is: 0.9506175945

<bound method LogisticRegression.get_params of LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)>
Accuracy score for test is: 0.965458
Strength is: 1.00
F-1 score(micro) for test is: 0.9654576857
F-1 score(macro) for test is: 0.9601777022

<bound method LogisticRegression.get_params of LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False)>
Accuracy score for test is: 0.972366
Strength is: 1.00
F-1 score(micro) for test is: 0.9723661485
F-1 score(macro) for test is: 0.9643978900


NO CROSS VALIDATION DECISION tree
\\ random_state = 0
the criteria is: gini
('the mean accuracy is: %.10f', 0.9084628670120898)
the criteria is: entropy
('the mean accuracy is: %.10f', 0.9205526770293609)

\\no specify random_state
the criteria is: entropy
('the mean accuracy is: %.10f', 0.92573402417962)
the criteria is: gini
('the mean accuracy is: %.10f', 0.9015544041450777)


\\f-1 score version, random_state = 0
the criteria is: entropy
the mean accuracy is: 0.9205526770
F-1 score(micro) for test is: 0.9205526770
F-1 score(macro) for test is: 0.9163969581

the criteria is: gini
the mean accuracy is: 0.9084628670
F-1 score(micro) for test is: 0.9084628670
F-1 score(macro) for test is: 0.8999685616

\\f-1 score version, no specify random_state
the criteria is: entropy
the mean accuracy is: 0.9274611399
F-1 score(micro) for test is: 0.9274611399
F-1 score(macro) for test is: 0.9254488734


the criteria is: gini
the mean accuracy is: 0.9050086356
F-1 score(micro) for test is: 0.9050086356
F-1 score(macro) for test is: 0.9044105818

---------------------------------------------
kNN MODEL RESULTS
kNN: p = 1 neighbors = 3
Weighted F-1 Score: 0.865378142899
kNN: p = 2 neighbors = 3
Weighted F-1 Score: 0.829934281361
kNN: p = 3 neighbors = 3
Weighted F-1 Score: 0.694501644175
kNN: p = 1 neighbors = 5
Weighted F-1 Score: 0.85262428158
kNN: p = 2 neighbors = 5
Weighted F-1 Score: 0.829480762182
kNN: p = 3 neighbors = 5
Weighted F-1 Score: 0.709450093452

RANDOM FOREST RESULTS (weighted F-1 scores)
low variance deleted: 0.944530707546
all features: 0.931073062552
entropy low variance deleted: 0.944638658764
entropy all features: 0.925094637113

LOGISTIC REGRESSION RESULTS (weighted F-1 scores)
penalty='l1', solver="saga", multi_class="multinomial": 0.9636952399
penalty='l2', solver="saga", multi_class="multinomial": 0.9653883325
penalty='l2', solver="saga", multi_class="ovr": 0.9706439371
penalty='l1', solver="saga", multi_class="ovr": 0.9672477667
penalty='l2', solver="lbfgs", multi_class="multinomial": 0.9569190286
penalty='l2', solver="lbfgs", multi_class="ovr": 0.9654189012
penalty='l2', solver="liblinear", multi_class="ovr": 0.9653660523
penalty='l1', solver="liblinear", multi_class="ovr": 0.9723302840

DECISION TREE RESULTS (weighted F1 scores)
gini low variance deleted: 0.9087803046
gini all features: 0.9119682849
entropy all features: 0.9223861431
entropy low variance deleted: 0.9257342661
Weighted F-1 Score:
